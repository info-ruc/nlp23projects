\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    \usepackage[UTF8]{ctex} % 用于中文
    \setCJKmainfont{SimSun} % 
    \setmainfont{Times New Roman}
    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{大语言模型微调}
    \author{罗从良(23级秋大数据)}
    
    
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    本文针对大语言模型llama2微调进行操作讲述。
    llama2是meta最新开源的语言大模型，训练数据集达2万亿token，上下文长度是由llama的2048扩展到4096，
    可以理解和生成更长的文本，包括7B、13B和70B三个模型，在各种基准集的测试上表现突出。
    而且该模型可用于研究和商业用途。
    这对于我们使用llama2进行微调是非常友好的。我们这里所说的微调实际上是在特定的任务上进行精细化训练,从而提高模型的性能。当然也有一种
    预训练微调，这个相对对资源和数据量有一定的要求。

    本次微调使用了4张V100进行训练。训练过程中发现大约20多G便可完成训练。所以一种32G的卡是可以完成微调训练的。

    本次微调是使用Llama-2-7b-hf模型作为基座模型，如果用来做对话的话，可以使用LLama-2-7b-chat作为基座模型。

    在论文"Data Quality Is All You Need。"，  MetaAI进行实验时发现，
    少量高质量数据集训练模型的效果，要好于大量低质量数据集的训练效果。
    因此进行SFT时候，可以不要一味地追求量，有时质量更为重要。

    微调时初始学习率为2e-4，并采用余弦学习率下降，权重衰减为0.1，
    训练批次大小为64，最大长度为4096。为了提高模型训练效率，
    将多组数据进行拼接，尽量填满4096，每条数据直接用停止符隔开，计算loss时仅计算每条样本target内容的loss。

    总的来说,Llama2微调技术让大型语言模型可以高效地迁移到各种下游任务,同时保持通用语言理解的能力,
    是当前自然语言处理领域的热点技术之一。随着计算能力的增强和参数量的扩大,这类技术还有很大的改进空间。

    以下我使用使用vscode利用jupyter插件进行微调训练操作。

    它包括了python环境、模块或包引入， 数据预处理、bnb机制处理、LoRa方法操作等等。
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{!}which\PY{+w}{ }python
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
/data/lcl/anaconda3/envs/chinese\_llama2/bin/python
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{!}nvidia\PYZhy{}smi
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
huggingface/tokenizers: The current process just got forked, after parallelism
has already been used. Disabling parallelism to avoid deadlocks{\ldots}
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS\_PARALLELISM=(true |
false)
Mon Dec 11 08:27:49 2023
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.67       Driver Version: 460.67       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE{\ldots}  Off  | 00000000:3B:00.0 Off |                    0 |
| N/A   37C    P0    35W / 250W |   2299MiB / 32510MiB |      0\%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE{\ldots}  Off  | 00000000:86:00.0 Off |                    0 |
| N/A   38C    P0    34W / 250W |   2469MiB / 32510MiB |      0\%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE{\ldots}  Off  | 00000000:AF:00.0 Off |                    0 |
| N/A   36C    P0    37W / 250W |   2521MiB / 32510MiB |      0\%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE{\ldots}  Off  | 00000000:D8:00.0 Off |                    0 |
| N/A   38C    P0    33W / 250W |   2307MiB / 32510MiB |      0\%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     66429      C   {\ldots}chinese\_llama2/bin/python     2167MiB |
|    1   N/A  N/A     66429      C   {\ldots}chinese\_llama2/bin/python     2337MiB |
|    2   N/A  N/A     66429      C   {\ldots}chinese\_llama2/bin/python     2389MiB |
|    3   N/A  N/A     66429      C   {\ldots}chinese\_llama2/bin/python     2175MiB |
+-----------------------------------------------------------------------------+
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{argparse} 
\PY{k+kn}{import} \PY{n+nn}{bitsandbytes} \PY{k}{as} \PY{n+nn}{bnb} 
\PY{k+kn}{from} \PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{load\PYZus{}dataset}
\PY{k+kn}{from} \PY{n+nn}{functools} \PY{k+kn}{import} \PY{n}{partial}
\PY{k+kn}{import} \PY{n+nn}{os}
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{k+kn}{from} \PY{n+nn}{peft} \PY{k+kn}{import} \PY{n}{LoraConfig}\PY{p}{,} \PY{n}{get\PYZus{}peft\PYZus{}model}\PY{p}{,} \PY{n}{prepare\PYZus{}model\PYZus{}for\PYZus{}kbit\PYZus{}training}\PY{p}{,} \PY{n}{AutoPeftModelForSeq2SeqLM}
\PY{k+kn}{from} \PY{n+nn}{transformers} \PY{k+kn}{import} \PY{n}{AutoModelForCausalLM}\PY{p}{,} \PY{n}{AutoTokenizer}\PY{p}{,} \PY{n}{set\PYZus{}seed}\PY{p}{,} \PY{n}{Trainer}\PY{p}{,} \PY{n}{TrainingArguments}\PY{p}{,} \PY{n}{BitsAndBytesConfig}\PY{p}{,} \PYZbs{}
    \PY{n}{DataCollatorForLanguageModeling}\PY{p}{,} \PY{n}{Trainer}\PY{p}{,} \PY{n}{TrainingArguments}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{load\PYZus{}model}\PY{p}{(}\PY{n}{model\PYZus{}name}\PY{p}{,} \PY{n}{bnb\PYZus{}config}\PY{p}{)}\PY{p}{:}
    \PY{n}{n\PYZus{}gpus} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{device\PYZus{}count}\PY{p}{(}\PY{p}{)}
    \PY{n}{max\PYZus{}memory} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{l+m+mi}{40960}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{MB}\PY{l+s+s1}{\PYZsq{}}

    \PY{c+c1}{\PYZsh{} 正常情况下，我们直接加载model\PYZus{}name,可以查找一下资料关于quantization,device\PYZus{}map,max\PYZus{}memory等参数}
    \PY{n}{model} \PY{o}{=} \PY{n}{AutoModelForCausalLM}\PY{o}{.}\PY{n}{from\PYZus{}pretrained}\PY{p}{(}
        \PY{n}{model\PYZus{}name}\PY{p}{,}
        \PY{n}{quantization\PYZus{}config}\PY{o}{=}\PY{n}{bnb\PYZus{}config}\PY{p}{,}
        \PY{n}{device\PYZus{}map}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{c+c1}{\PYZsh{} 高效地将模型分配给可用资源。}
        \PY{n}{max\PYZus{}memory}\PY{o}{=}\PY{p}{\PYZob{}}\PY{n}{i}\PY{p}{:} \PY{n}{max\PYZus{}memory} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}gpus}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{,}
    \PY{p}{)}
    \PY{n}{tokenizer} \PY{o}{=} \PY{n}{AutoTokenizer}\PY{o}{.}\PY{n}{from\PYZus{}pretrained}\PY{p}{(}\PY{n}{model\PYZus{}name}\PY{p}{,} \PY{n}{use\PYZus{}auth\PYZus{}token}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Needed for LLaMA tokenizer}
    \PY{n}{tokenizer}\PY{o}{.}\PY{n}{pad\PYZus{}token} \PY{o}{=} \PY{n}{tokenizer}\PY{o}{.}\PY{n}{eos\PYZus{}token}

    \PY{k}{return} \PY{n}{model}\PY{p}{,} \PY{n}{tokenizer}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{datasets} \PY{k+kn}{import} \PY{n}{load\PYZus{}dataset}

\PY{n}{dataset} \PY{o}{=} \PY{n}{load\PYZus{}dataset}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/data/lcl/LLM/llama/datasets}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{split}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
HF google storage unreachable. Downloading and preparing it from source
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
Downloading data files:   0\%|          | 0/1 [00:00<?, ?it/s]
    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
Extracting data files:   0\%|          | 0/1 [00:00<?, ?it/s]
    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
Generating train split: 0 examples [00:00, ? examples/s]
    \end{Verbatim}

    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} 查看数据集}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of prompts: }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{dataset}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{dataset}\PY{p}{,} \PY{n+nb}{type}\PY{p}{(}\PY{n}{dataset}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Number of prompts: 15011
Dataset(\{
    features: ['id', 'category', 'instruction\_zh', 'context\_zh', 'response',
'instruction', 'context'],
    num\_rows: 15011
\}) <class 'datasets.arrow\_dataset.Dataset'>
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} 探索数据集里具体的内容}
\PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n}{dataset}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{e}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{e}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{instruction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
\{'id': 1635, 'category': 'information\_extraction', 'instruction\_zh':
'根据以下关于瑞典经济的文章，哪个经济部门占据了最大的产出？', 'context\_zh': '瑞典是一个出口导向的混合经济体，拥有现代化的分销系统、优秀的内外
部通讯和熟练的劳动力。木材、水力和铁矿石构成了一个经济体的资源基础，这个经济体严重依赖对外贸易。瑞典的工程部门占产出和出口的50\%。电信、汽车工业和制药工业也非
常重要。农业占GDP和就业的2\%。军火工业具有高度先进的技术声誉。', 'response': 'According to this passage, the
engineering sector accounts for the largest output, generating 50\% of output and
exports.', 'instruction': 'Based on the following passage regarding the economy
of Sweden, what is the economic sector that accounts for the largest output?',
'context': "Sweden is an export-oriented mixed economy featuring a modern
distribution system, excellent internal and external communications, and a
skilled labor force. Timber, hydropower and iron ore constitute the resource
base of an economy heavily oriented toward foreign trade. Sweden's engineering
sector accounts for 50\% of output and exports. Telecommunications, the
automotive industry and the pharmaceutical industries are also of great
importance. Agriculture accounts for 2 percent of GDP and employment. The
armaments industry has a technologically highly advanced reputation."\}
Based on the following passage regarding the economy of Sweden, what is the
economic sector that accounts for the largest output?
\{'id': 4959, 'category': 'classification', 'instruction\_zh': '`这些国家是共产主义国家吗：阿富汗、
阿尔巴尼亚、阿尔及利亚、安道尔、安哥拉、安提瓜和巴布达、阿根廷、亚美尼亚、澳大利亚、奥地利、阿塞拜疆、巴哈马、巴林、孟加拉国、巴巴多斯、白俄罗斯、比利时、伯利兹
、贝宁、不丹、玻利维亚、波斯尼亚和黑塞哥维那、博茨瓦纳、巴西、文莱、保加利亚、布基纳法索、布隆迪、科特迪瓦、佛得角、柬埔寨、喀麦隆、加拿大、中非共和国、乍得、智
利、中国、哥伦比亚、科摩罗、刚果（布）、哥斯达黎加、克罗地亚、古巴、塞浦路斯、捷克共和国、刚果（金）、丹麦、吉布提、多米尼克、多米尼加共和国、厄瓜多尔、埃及、萨
尔瓦多、赤道几内亚、厄立特里亚、爱沙尼亚、斯威士兰、埃塞俄比亚、斐济、芬兰、法国、加蓬、冈比亚、格鲁吉亚、德国、加纳、希腊、格林纳达、危地马拉、几内亚、几内亚比
绍、圭亚那、海地、梵蒂冈、洪都拉斯、匈牙利、冰岛、印度、印度尼西亚、伊朗、伊拉克、爱尔兰、以色列、意大利、牙买加、日本、约旦、哈萨克斯坦、肯尼亚、基里巴斯、科威
特、吉尔吉斯斯坦、老挝、拉脱维亚、黎巴嫩、莱索托、利比里亚、利比亚、列支敦士登、立陶宛、卢森堡、马达加斯加、马拉维、马来西亚、马尔代夫、马里、马耳他、马绍尔群岛
、毛里塔尼亚、毛里求斯、墨西哥、密克罗尼西亚、摩尔多瓦、摩纳哥、蒙古、黑山、摩洛哥、莫桑比克、缅甸（前缅甸）、纳米比亚、瑙鲁、尼泊尔、荷兰、新西兰、尼加拉瓜、尼
日尔、尼日利亚、朝鲜、北马其顿、挪威、阿曼、巴基斯坦、帕劳、巴勒斯坦国、巴拿马、巴布亚新几内亚、巴拉圭、秘鲁、菲律宾、波兰、葡萄牙、卡塔尔、罗马尼亚、俄罗斯、卢
旺达、圣基茨和尼维斯、圣卢西亚、圣文森特和格林纳丁斯、萨摩亚、圣马力诺、圣多美和普林西比、沙特阿拉伯、塞内加尔、塞尔维亚、塞舌尔、塞拉利昂、新加坡、斯洛伐克、斯
洛文尼亚、所罗门群岛、索马里、南非、韩国、南苏丹、西班牙、斯里兰卡、苏丹、苏里南、瑞典、瑞士、叙利亚、塔吉克斯坦、坦桑尼亚、泰国、东帝汶、多哥、汤加、特立尼达和
多巴哥、突尼斯、土耳其、土库曼斯坦、', 'context\_zh': '', 'response': 'China, Cuba, Laos, Vietnam,
North Korea', 'instruction': "Are they communist countries: Afghanistan,
Albania, Algeria, Andorra, Angola, Antigua and Barbuda, Argentina, Armenia,
Australia, Austria, Azerbaijan, Bahamas, Bahrain, Bangladesh, Barbados, Belarus,
Belgium, Belize, Benin, Bhutan, Bolivia, Bosnia and Herzegovina, Botswana,
Brazil, Brunei, Bulgaria, Burkina Faso, Burundi, Côte d'Ivoire, Cabo Verde,
Cambodia, Cameroon, Canada, Central African Republic, Chad, Chile, China,
Colombia, Comoros, Congo (Congo-Brazzaville), Costa Rica, Croatia, Cuba, Cyprus,
Czechia (Czech Republic), Democratic Republic of the Congo, Denmark, Djibouti,
Dominica, Dominican Republic, Ecuador, Egypt, El Salvador, Equatorial Guinea,
Eritrea, Estonia, Eswatini , Ethiopia, Fiji, Finland, France, Gabon, Gambia,
Georgia, Germany, Ghana, Greece, Grenada, Guatemala, Guinea, Guinea-Bissau,
Guyana, Haiti, Holy See, Honduras, Hungary, Iceland, India, Indonesia, Iran,
Iraq, Ireland, Israel, Italy, Jamaica, Japan, Jordan, Kazakhstan, Kenya,
Kiribati, Kuwait, Kyrgyzstan, Laos, Latvia, Lebanon, Lesotho, Liberia, Libya,
Liechtenstein, Lithuania, Luxembourg, Madagascar, Malawi, Malaysia, Maldives,
Mali, Malta, Marshall Islands, Mauritania, Mauritius, Mexico, Micronesia,
Moldova, Monaco, Mongolia, Montenegro, Morocco, Mozambique, Myanmar (formerly
Burma), Namibia, Nauru, Nepal, Netherlands, New Zealand, Nicaragua, Niger,
Nigeria, North Korea, North Macedonia, Norway, Oman, Pakistan, Palau, Palestine
State, Panama, Papua New Guinea, Paraguay, Peru, Philippines, Poland, Portugal,
Qatar, Romania, Russia, Rwanda, Saint Kitts and Nevis, Saint Lucia, Saint
Vincent and the Grenadines, Samoa, San Marino, Sao Tome and Principe, Saudi
Arabia, Senegal, Serbia, Seychelles, Sierra Leone, Singapore, Slovakia,
Slovenia, Solomon Islands, Somalia, South Africa, South Korea, South Sudan,
Spain, Sri Lanka, Sudan, Suriname, Sweden, Switzerland, Syria, Tajikistan,
Tanzania, Thailand, Timor-Leste, Togo, Tonga, Trinidad and Tobago, Tunisia,
Turkey, Turkmenistan, Tuvalu, Uganda, Ukraine, United Arab Emirates, United
Kingdom, United States of America, Uruguay, Uzbekistan, Vanuatu, Venezuela,
Vietnam, Yemen, Zambia, Zimbabwe", 'context': ''\}
Are they communist countries: Afghanistan, Albania, Algeria, Andorra, Angola,
Antigua and Barbuda, Argentina, Armenia, Australia, Austria, Azerbaijan,
Bahamas, Bahrain, Bangladesh, Barbados, Belarus, Belgium, Belize, Benin, Bhutan,
Bolivia, Bosnia and Herzegovina, Botswana, Brazil, Brunei, Bulgaria, Burkina
Faso, Burundi, Côte d'Ivoire, Cabo Verde, Cambodia, Cameroon, Canada, Central
African Republic, Chad, Chile, China, Colombia, Comoros, Congo (Congo-
Brazzaville), Costa Rica, Croatia, Cuba, Cyprus, Czechia (Czech Republic),
Democratic Republic of the Congo, Denmark, Djibouti, Dominica, Dominican
Republic, Ecuador, Egypt, El Salvador, Equatorial Guinea, Eritrea, Estonia,
Eswatini , Ethiopia, Fiji, Finland, France, Gabon, Gambia, Georgia, Germany,
Ghana, Greece, Grenada, Guatemala, Guinea, Guinea-Bissau, Guyana, Haiti, Holy
See, Honduras, Hungary, Iceland, India, Indonesia, Iran, Iraq, Ireland, Israel,
Italy, Jamaica, Japan, Jordan, Kazakhstan, Kenya, Kiribati, Kuwait, Kyrgyzstan,
Laos, Latvia, Lebanon, Lesotho, Liberia, Libya, Liechtenstein, Lithuania,
Luxembourg, Madagascar, Malawi, Malaysia, Maldives, Mali, Malta, Marshall
Islands, Mauritania, Mauritius, Mexico, Micronesia, Moldova, Monaco, Mongolia,
Montenegro, Morocco, Mozambique, Myanmar (formerly Burma), Namibia, Nauru,
Nepal, Netherlands, New Zealand, Nicaragua, Niger, Nigeria, North Korea, North
Macedonia, Norway, Oman, Pakistan, Palau, Palestine State, Panama, Papua New
Guinea, Paraguay, Peru, Philippines, Poland, Portugal, Qatar, Romania, Russia,
Rwanda, Saint Kitts and Nevis, Saint Lucia, Saint Vincent and the Grenadines,
Samoa, San Marino, Sao Tome and Principe, Saudi Arabia, Senegal, Serbia,
Seychelles, Sierra Leone, Singapore, Slovakia, Slovenia, Solomon Islands,
Somalia, South Africa, South Korea, South Sudan, Spain, Sri Lanka, Sudan,
Suriname, Sweden, Switzerland, Syria, Tajikistan, Tanzania, Thailand, Timor-
Leste, Togo, Tonga, Trinidad and Tobago, Tunisia, Turkey, Turkmenistan, Tuvalu,
Uganda, Ukraine, United Arab Emirates, United Kingdom, United States of America,
Uruguay, Uzbekistan, Vanuatu, Venezuela, Vietnam, Yemen, Zambia, Zimbabwe
\{'id': 3858, 'category': 'classification', 'instruction\_zh': '我正在为我的婚礼注册礼物，需要包括一
些在我新家有用的物品。哪些物品是可以作为婚礼礼物的家庭用品：搅拌机、咖啡机、公交车票、毛巾、床单、滑板、手机、汽车、餐具、银器、健身会员卡、玻璃器皿、礼服、刹车
片、自行车、相框。', 'context\_zh': '', 'response': 'A blender, coffee maker, towels,
sheets, dishes, silverware, glassware, picture frames are examples of household
items that can be given as a wedding gift.', 'instruction': 'I am registering
for gifts for my wedding and need to include items that would be useful in my
new home. Which items are household items that can be given as a wedding gift:
blender, coffee maker, bus fare, towels, sheets, skateboard, cell phone, car,
dishes, silverware, gym membership, glassware, tuxedo, brake pads, bicycle,
picture frames', 'context': ''\}
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} 预处理数据集}

\PY{c+c1}{\PYZsh{} instrcuction fine\PYZhy{}tuning(指令微调)是一种常用的技术，用于针对特定的下游用例微调基础LLM。}

\PY{c+c1}{\PYZsh{} 按如下方式设置提示的格式：}
\end{Verbatim}
\end{tcolorbox}

    \begin{verbatim}
Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
Sea or Mountain

### Response:
I believe Mountain are more attractive but Ocean has it's own beauty and this tropical weather definitely turn you on! SO 50% 50%

### End
\end{verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} 通过主题标签分隔每个提示部分}
\PY{k}{def} \PY{n+nf}{create\PYZus{}prompt\PYZus{}formats}\PY{p}{(}\PY{n}{sample}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        格式化为（\PYZsq{}instruction\PYZsq{}, \PYZsq{}context\PYZsq{}, \PYZsq{}response\PYZsq{}）}
\PY{l+s+sd}{        然后组成为新的特征行。}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{INTRO\PYZus{}BLURB} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Below is an instruction that describes a task. Write a response that appropriately completes the request.}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{INSTRUCTION\PYZus{}KEY} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}\PYZsh{}\PYZsh{} Instruction:}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{INPUT\PYZus{}KEY} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Input:}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{RESPONSE\PYZus{}KEY} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}\PYZsh{}\PYZsh{} Response:}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{END\PYZus{}KEY} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}\PYZsh{}\PYZsh{} End}\PY{l+s+s2}{\PYZdq{}}

    \PY{n}{blurb} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{INTRO\PYZus{}BLURB}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{instruction} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{INSTRUCTION\PYZus{}KEY}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+si}{\PYZob{}}\PY{n}{sample}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{instruction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{input\PYZus{}context} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{INPUT\PYZus{}KEY}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+si}{\PYZob{}}\PY{n}{sample}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{context}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{n}{sample}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{context}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{else} \PY{k+kc}{None} 
    \PY{n}{response} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{RESPONSE\PYZus{}KEY}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+si}{\PYZob{}}\PY{n}{sample}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{response}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{end} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{END\PYZus{}KEY}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}

    \PY{n}{parts} \PY{o}{=} \PY{p}{[}\PY{n}{part} \PY{k}{for} \PY{n}{part} \PY{o+ow}{in} \PY{p}{[}\PY{n}{blurb}\PY{p}{,} \PY{n}{instruction}\PY{p}{,} \PY{n}{input\PYZus{}context}\PY{p}{,} \PY{n}{response}\PY{p}{,} \PY{n}{end}\PY{p}{]} \PY{k}{if} \PY{n}{part}\PY{p}{]}

    \PY{n}{formatted\PYZus{}prompt} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{parts}\PY{p}{)}
    
    \PY{n}{sample}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{text}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{n}{formatted\PYZus{}prompt}

    \PY{k}{return} \PY{n}{sample}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} 使用model tokenizer 将这些prompts into tokenized ones}
\PY{c+c1}{\PYZsh{} 目标是创建长度均匀的输入序列（适用于微调语言模型，因为它可以最大限度地提高效率并最小化计算开销），并且不得超过模型的最大标记限制。}

\PY{k}{def} \PY{n+nf}{get\PYZus{}max\PYZus{}length}\PY{p}{(}\PY{n}{model}\PY{p}{)}\PY{p}{:}
    \PY{n}{conf} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{config}
    \PY{n}{max\PYZus{}length} \PY{o}{=} \PY{k+kc}{None}
    \PY{k}{for} \PY{n}{length\PYZus{}setting} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}positions}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{max\PYZus{}position\PYZus{}embeddings}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{seq\PYZus{}length}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{:}
        \PY{n}{max\PYZus{}length} \PY{o}{=} \PY{n+nb}{getattr}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{config}\PY{p}{,} \PY{n}{length\PYZus{}setting}\PY{p}{,} \PY{k+kc}{None}\PY{p}{)}
        \PY{k}{if} \PY{n}{max\PYZus{}length}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{找到最大长度：}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}length}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{break}
    \PY{k}{if} \PY{o+ow}{not} \PY{n}{max\PYZus{}length}\PY{p}{:}
        \PY{n}{max\PYZus{}length} \PY{o}{=} \PY{l+m+mi}{1024}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{使用默认最大长度：}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}length}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{max\PYZus{}length} 

\PY{c+c1}{\PYZsh{} 准备batch}
\PY{c+c1}{\PYZsh{} 对一个批次的数据进行预处理}
\PY{k}{def} \PY{n+nf}{preprocess\PYZus{}batch}\PY{p}{(}\PY{n}{batch}\PY{p}{,} \PY{n}{tokenizer}\PY{p}{,} \PY{n}{max\PYZus{}length}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        Tokenizer a batch}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} batch[\PYZdq{}text\PYZdq{}]表示批次中的文本数据，max\PYZus{}length表示标记化后每个文本序列的最大长度，trucation=True表示文本长度超过了max\PYZus{}length则进行截取}
    \PY{c+c1}{\PYZsh{} 返回标记后的文本}
    \PY{k}{return} \PY{n}{tokenizer}\PY{p}{(}
        \PY{n}{batch}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{text}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
        \PY{n}{max\PYZus{}length} \PY{o}{=} \PY{n}{max\PYZus{}length}\PY{p}{,}
        \PY{n}{truncation}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
    \PY{p}{)}

\PY{c+c1}{\PYZsh{} 准备数据集}
\PY{k}{def} \PY{n+nf}{preprocess\PYZus{}dataset}\PY{p}{(}\PY{n}{tokenizer}\PY{p}{:} \PY{n}{AutoTokenizer}\PY{p}{,} \PY{n}{max\PYZus{}length}\PY{p}{:} \PY{n+nb}{int}\PY{p}{,} \PY{n}{seed}\PY{p}{,} \PY{n}{dataset}\PY{p}{:} \PY{n+nb}{str}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        准备数据集}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{准备数据集}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{dataset} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{create\PYZus{}prompt\PYZus{}formats}\PY{p}{)}\PY{c+c1}{\PYZsh{},batched=True)}

    \PY{c+c1}{\PYZsh{} 对数据集中的每个batch进行预处理，移除\PYZsq{}instruction\PYZsq{}, \PYZsq{}context\PYZsq{}, \PYZsq{}response\PYZsq{}, \PYZsq{}category\PYZsq{}}
    \PY{c+c1}{\PYZsh{} from functools import partial}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{        def add(x, y):}
\PY{l+s+sd}{            return x + y}

\PY{l+s+sd}{        add\PYZus{}5 = partial(add, 5)}
\PY{l+s+sd}{        print(add\PYZus{}5(3)) \PYZsh{} 输出8}
\PY{l+s+sd}{        print(add\PYZus{}5(7)) \PYZsh{} 输出12}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{\PYZus{}preprocessing\PYZus{}function} \PY{o}{=} \PY{n}{partial}\PY{p}{(}\PY{n}{preprocess\PYZus{}batch}\PY{p}{,} \PY{n}{max\PYZus{}length}\PY{o}{=}\PY{n}{max\PYZus{}length}\PY{p}{,} \PY{n}{tokenizer}\PY{o}{=}\PY{n}{tokenizer}\PY{p}{)}
    \PY{n}{dataset} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{map}\PY{p}{(}
        \PY{n}{\PYZus{}preprocessing\PYZus{}function}\PY{p}{,}
        \PY{n}{batched}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
        \PY{n}{remove\PYZus{}columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{instruction}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{context}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{response}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{text}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{category}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
    \PY{p}{)}

    \PY{c+c1}{\PYZsh{} Filter out samples that have input\PYZus{}ids exceeding max\PYZus{}length}
    \PY{n}{dataset} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{filter}\PY{p}{(}\PY{k}{lambda} \PY{n}{sample}\PY{p}{:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{sample}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{input\PYZus{}ids}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{max\PYZus{}length}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Shuffle dataset}
    \PY{n}{dataset} \PY{o}{=} \PY{n}{dataset}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{seed}\PY{o}{=}\PY{n}{seed}\PY{p}{)}

    \PY{k}{return} \PY{n}{dataset}
    
\end{Verbatim}
\end{tcolorbox}

    创建bitsandbytes配置，这将允许我们以 4 位加载我们的
LLM。这样，我们可以将使用的内存除以
4，并在较小的设备上导入模型。为了节省内存，我们选择应用 bfloat16
计算数据类型和嵌套量化。

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{create\PYZus{}bnb\PYZus{}config}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{bnb\PYZus{}config} \PY{o}{=} \PY{n}{BitsAndBytesConfig}\PY{p}{(}
        \PY{n}{load\PYZus{}in\PYZus{}4bit}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
        \PY{n}{bnb\PYZus{}4bit\PYZus{}use\PYZus{}double\PYZus{}quant}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
        \PY{n}{bnb\PYZus{}4bit\PYZus{}quant\PYZus{}type}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nf4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
        \PY{n}{bnb\PYZus{}4bit\PYZus{}compute\PYZus{}dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{bfloat16}\PY{p}{,}
    \PY{p}{)}
    \PY{k}{return} \PY{n}{bnb\PYZus{}config}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} 为了使用LoRa方法，我们需要将模型包装为PeftModel}
\PY{c+c1}{\PYZsh{} 因此，我们需要实现LoRa配置}
\PY{c+c1}{\PYZsh{} 函数需要目标模块来更新必要的矩阵。}

\PY{k}{def} \PY{n+nf}{create\PYZus{}peft\PYZus{}config}\PY{p}{(}\PY{n}{modules}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} }
\PY{l+s+sd}{        为您的模型创建参数高效的微调配置}
\PY{l+s+sd}{    :param modules: 要应用Lora的模块的名称}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{config} \PY{o}{=} \PY{n}{LoraConfig}\PY{p}{(}
        \PY{n}{r}\PY{o}{=}\PY{l+m+mi}{16}\PY{p}{,} \PY{c+c1}{\PYZsh{}更新矩阵维度16}
        \PY{n}{lora\PYZus{}alpha}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} \PY{c+c1}{\PYZsh{} 表示缩放参数为64}
        \PY{n}{target\PYZus{}modules}\PY{o}{=}\PY{n}{modules}\PY{p}{,} \PY{c+c1}{\PYZsh{}表示要应用Lora的模块名称}
        \PY{n}{lora\PYZus{}dropout}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{c+c1}{\PYZsh{}  layers的dropout概率为0.1}
        \PY{n}{bias}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{none}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{c+c1}{\PYZsh{} 表示偏差为none}
        \PY{n}{task\PYZus{}type}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{CAUSAL\PYZus{}LM}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
    \PY{p}{)}
    \PY{k}{return} \PY{n}{config}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} 函数用来获取lora操作系列模块}
\PY{k}{def} \PY{n+nf}{find\PYZus{}all\PYZus{}linear\PYZus{}names}\PY{p}{(}\PY{n}{model}\PY{p}{)}\PY{p}{:}
    \PY{n+nb+bp}{cls} \PY{o}{=} \PY{n}{bnb}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{Linear4bit} \PY{c+c1}{\PYZsh{}if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)}
    \PY{n}{lora\PYZus{}module\PYZus{}names} \PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{p}{)}
    \PY{k}{for} \PY{n}{name}\PY{p}{,} \PY{n}{module} \PY{o+ow}{in} \PY{n}{model}\PY{o}{.}\PY{n}{named\PYZus{}modules}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{n+nb}{isinstance}\PY{p}{(}\PY{n}{module}\PY{p}{,} \PY{n+nb+bp}{cls}\PY{p}{)}\PY{p}{:}
            \PY{n}{names} \PY{o}{=} \PY{n}{name}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{lora\PYZus{}module\PYZus{}names}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{names}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{names}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{1} \PY{k}{else} \PY{n}{names}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}

    \PY{k}{if} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lm\PYZus{}head}\PY{l+s+s1}{\PYZsq{}} \PY{o+ow}{in} \PY{n}{lora\PYZus{}module\PYZus{}names}\PY{p}{:}  \PY{c+c1}{\PYZsh{} needed for 16\PYZhy{}bit}
        \PY{n}{lora\PYZus{}module\PYZus{}names}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lm\PYZus{}head}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{k}{return} \PY{n+nb}{list}\PY{p}{(}\PY{n}{lora\PYZus{}module\PYZus{}names}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} 创建一个辅助函数用来查看模型中的可训练参数}
\PY{k}{def} \PY{n+nf}{print\PYZus{}trainable\PYZus{}parameters}\PY{p}{(}\PY{n}{model}\PY{p}{,}\PY{n}{use\PYZus{}4bit}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} }
\PY{l+s+sd}{        打印模型中的训练参数}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{trainable\PYZus{}params} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{all\PYZus{}param} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{param} \PY{o+ow}{in} \PY{n}{model}\PY{o}{.}\PY{n}{named\PYZus{}parameters}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{n}{num\PYZus{}params} \PY{o}{=} \PY{n}{param}\PY{o}{.}\PY{n}{numel}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} 如果使用DS Zero 3并且权重初始化为空。}
        \PY{k}{if} \PY{n}{num\PYZus{}params} \PY{o}{==} \PY{l+m+mi}{0} \PY{o+ow}{and} \PY{n+nb}{hasattr}\PY{p}{(}\PY{n}{param}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ds\PYZus{}numel}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
            \PY{n}{num\PYZus{}params} \PY{o}{=} \PY{n}{param}\PY{o}{.}\PY{n}{ds\PYZus{}numel}
        
        \PY{n}{all\PYZus{}param} \PY{o}{+}\PY{o}{=} \PY{n}{num\PYZus{}params}
        \PY{k}{if} \PY{n}{param}\PY{o}{.}\PY{n}{requires\PYZus{}grad}\PY{p}{:}
            \PY{n}{trainable\PYZus{}params} \PY{o}{+}\PY{o}{=} \PY{n}{num\PYZus{}params}
    \PY{k}{if} \PY{n}{use\PYZus{}4bit}\PY{p}{:}
        \PY{n}{trainable\PYZus{}params} \PY{o}{/}\PY{o}{=} \PY{l+m+mi}{2}
    \PY{c+c1}{\PYZsh{} print(f\PYZdq{}all params: \PYZob{}all\PYZus{}param:, d\PYZcb{} || trainable params: \PYZob{}trainable\PYZus{}params:, d\PYZcb{} || trainable\PYZpc{}: \PYZob{}100 * trainable\PYZus{}params / all\PYZus{}param\PYZcb{}\PYZdq{})}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} 加载预训练模型和配置}
\PY{n}{model\PYZus{}name} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{meta\PYZhy{}llama/Llama\PYZhy{}2\PYZhy{}7b\PYZhy{}hf}\PY{l+s+s2}{\PYZdq{}} 

\PY{n}{bnb\PYZus{}config} \PY{o}{=} \PY{n}{create\PYZus{}bnb\PYZus{}config}\PY{p}{(}\PY{p}{)}

\PY{n}{model}\PY{p}{,} \PY{n}{tokenizer} \PY{o}{=} \PY{n}{load\PYZus{}model}\PY{p}{(}\PY{n}{model\PYZus{}name}\PY{p}{,} \PY{n}{bnb\PYZus{}config}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
Loading checkpoint shards:   0\%|          | 0/2 [00:00<?, ?it/s]
    \end{Verbatim}

    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} 准备数据集}
\PY{n}{max\PYZus{}length} \PY{o}{=} \PY{n}{get\PYZus{}max\PYZus{}length}\PY{p}{(}\PY{n}{model}\PY{p}{)}
\PY{n}{dataset} \PY{o}{=} \PY{n}{preprocess\PYZus{}dataset}\PY{p}{(}\PY{n}{tokenizer}\PY{p}{,} \PY{n}{max\PYZus{}length}\PY{p}{,} \PY{l+m+mi}{515}\PY{p}{,} \PY{n}{dataset}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
找到最大长度：4096
准备数据集
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
Map:   0\%|          | 0/15011 [00:00<?, ? examples/s]
    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
Map:   0\%|          | 0/15011 [00:00<?, ? examples/s]
    \end{Verbatim}

    
    
    \begin{Verbatim}[commandchars=\\\{\}]
Filter:   0\%|          | 0/15011 [00:00<?, ? examples/s]
    \end{Verbatim}

    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{tokenizer}\PY{p}{,} \PY{n}{dataset}\PY{p}{,} \PY{n}{output\PYZus{}dir}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} 预处理启动梯度检查点技术，以在微调过程中减少内存使用}
    \PY{n}{model}\PY{o}{.}\PY{n}{gradient\PYZus{}checkpointing\PYZus{}enable}\PY{p}{(}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} 2 \PYZhy{} Using the prepare\PYZus{}model\PYZus{}for\PYZus{}kbit\PYZus{}training method from PEFT}
    \PY{n}{model} \PY{o}{=} \PY{n}{prepare\PYZus{}model\PYZus{}for\PYZus{}kbit\PYZus{}training}\PY{p}{(}\PY{n}{model}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Get lora module names}
    \PY{n}{modules} \PY{o}{=} \PY{n}{find\PYZus{}all\PYZus{}linear\PYZus{}names}\PY{p}{(}\PY{n}{model}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Create PEFT config for these modules and wrap the model to PEFT}
    \PY{n}{peft\PYZus{}config} \PY{o}{=} \PY{n}{create\PYZus{}peft\PYZus{}config}\PY{p}{(}\PY{n}{modules}\PY{p}{)}
    \PY{n}{model} \PY{o}{=} \PY{n}{get\PYZus{}peft\PYZus{}model}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{peft\PYZus{}config}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Print information about the percentage of trainable parameters}
    \PY{n}{print\PYZus{}trainable\PYZus{}parameters}\PY{p}{(}\PY{n}{model}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Training parameters}
    \PY{n}{trainer} \PY{o}{=} \PY{n}{Trainer}\PY{p}{(}
        \PY{n}{model}\PY{o}{=}\PY{n}{model}\PY{p}{,}
        \PY{n}{train\PYZus{}dataset}\PY{o}{=}\PY{n}{dataset}\PY{p}{,}
        \PY{n}{args}\PY{o}{=}\PY{n}{TrainingArguments}\PY{p}{(}
            \PY{n}{per\PYZus{}device\PYZus{}train\PYZus{}batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
            \PY{n}{gradient\PYZus{}accumulation\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,}
            \PY{n}{warmup\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,}
            \PY{n}{max\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,}
            \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{2e\PYZhy{}4}\PY{p}{,}
            \PY{n}{fp16}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
            \PY{n}{logging\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
            \PY{n}{output\PYZus{}dir}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{outputs}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
            \PY{n}{optim}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{paged\PYZus{}adamw\PYZus{}8bit}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
        \PY{p}{)}\PY{p}{,}
        \PY{n}{data\PYZus{}collator}\PY{o}{=}\PY{n}{DataCollatorForLanguageModeling}\PY{p}{(}\PY{n}{tokenizer}\PY{p}{,} \PY{n}{mlm}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
    \PY{p}{)}
    
    \PY{n}{model}\PY{o}{.}\PY{n}{config}\PY{o}{.}\PY{n}{use\PYZus{}cache} \PY{o}{=} \PY{k+kc}{False}  \PY{c+c1}{\PYZsh{} re\PYZhy{}enable for inference to speed up predictions for similar inputs}
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py}
    \PY{c+c1}{\PYZsh{} Verifying the datatypes before training}
    
    \PY{n}{dtypes} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
    \PY{k}{for} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{p} \PY{o+ow}{in} \PY{n}{model}\PY{o}{.}\PY{n}{named\PYZus{}parameters}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{n}{dtype} \PY{o}{=} \PY{n}{p}\PY{o}{.}\PY{n}{dtype}
        \PY{k}{if} \PY{n}{dtype} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{dtypes}\PY{p}{:} \PY{n}{dtypes}\PY{p}{[}\PY{n}{dtype}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{dtypes}\PY{p}{[}\PY{n}{dtype}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{p}\PY{o}{.}\PY{n}{numel}\PY{p}{(}\PY{p}{)}
    \PY{n}{total} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{n}{v} \PY{o+ow}{in} \PY{n}{dtypes}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:} \PY{n}{total}\PY{o}{+}\PY{o}{=} \PY{n}{v}
    \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{n}{v} \PY{o+ow}{in} \PY{n}{dtypes}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{v}\PY{o}{/}\PY{n}{total}\PY{p}{)}
     
    \PY{n}{do\PYZus{}train} \PY{o}{=} \PY{k+kc}{True}
    
    \PY{c+c1}{\PYZsh{} Launch training}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training...}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    
    \PY{k}{if} \PY{n}{do\PYZus{}train}\PY{p}{:}
        \PY{n}{train\PYZus{}result} \PY{o}{=} \PY{n}{trainer}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}
        \PY{n}{metrics} \PY{o}{=} \PY{n}{train\PYZus{}result}\PY{o}{.}\PY{n}{metrics}
        \PY{n}{trainer}\PY{o}{.}\PY{n}{log\PYZus{}metrics}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{metrics}\PY{p}{)}
        \PY{n}{trainer}\PY{o}{.}\PY{n}{save\PYZus{}metrics}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{train}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{metrics}\PY{p}{)}
        \PY{n}{trainer}\PY{o}{.}\PY{n}{save\PYZus{}state}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{metrics}\PY{p}{)}    
    
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}}
    
    \PY{c+c1}{\PYZsh{} Saving model}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Saving last checkpoint of the model...}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{os}\PY{o}{.}\PY{n}{makedirs}\PY{p}{(}\PY{n}{output\PYZus{}dir}\PY{p}{,} \PY{n}{exist\PYZus{}ok}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} 为了稍后加载和使用模型进行推理，我们使用了该 trainer.model.save\PYZus{}pretrained(output\PYZus{}dir) 函数，该函数保存了微调模型的权重、配置和分词器文件。}
    \PY{n}{trainer}\PY{o}{.}\PY{n}{model}\PY{o}{.}\PY{n}{save\PYZus{}pretrained}\PY{p}{(}\PY{n}{output\PYZus{}dir}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Free memory for merging weights}
    \PY{k}{del} \PY{n}{model}
    \PY{k}{del} \PY{n}{trainer}
    \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{empty\PYZus{}cache}\PY{p}{(}\PY{p}{)}





    
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{output\PYZus{}dir} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{results/llama2/final\PYZus{}checkpoint}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{train}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{tokenizer}\PY{p}{,} \PY{n}{dataset}\PY{p}{,} \PY{n}{output\PYZus{}dir}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0;
this can cause the process to hang. It is recommended to upgrade the kernel to
the minimum version or higher.
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
torch.float32 302387200 0.08541070604255438
torch.uint8 3238002688 0.9145892939574456
Training{\ldots}
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast
tokenizer, using the `\_\_call\_\_` method is faster than using a method to encode
the text followed by a call to the `pad` method to get a padded encoding.
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
<IPython.core.display.HTML object>
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
***** train metrics *****
  epoch                    =       0.01
  total\_flos               =   439355GF
  train\_loss               =     1.4391
  train\_runtime            = 0:01:14.31
  train\_samples\_per\_second =      1.076
  train\_steps\_per\_second   =      0.269
\{'train\_runtime': 74.3165, 'train\_samples\_per\_second': 1.076,
'train\_steps\_per\_second': 0.269, 'total\_flos': 471754134798336.0, 'train\_loss':
1.4391322791576386, 'epoch': 0.01\}
Saving last checkpoint of the model{\ldots}
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} 一旦我们有了微调的权重，我们就可以构建微调的模型，并将其保存到一个新目录，以及其关联的分词器。通过执行这些步骤，我们可以为推理提供内存高效的微调模型和分词器！}
\PY{k+kn}{from} \PY{n+nn}{peft} \PY{k+kn}{import} \PY{n}{AutoPeftModelForCausalLM}
\PY{n}{model} \PY{o}{=} \PY{n}{AutoPeftModelForCausalLM}\PY{o}{.}\PY{n}{from\PYZus{}pretrained}\PY{p}{(}\PY{n}{output\PYZus{}dir}\PY{p}{,} \PY{n}{device\PYZus{}map}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{auto}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{torch\PYZus{}dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{bfloat16}\PY{p}{)}
\PY{n}{model} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{merge\PYZus{}and\PYZus{}unload}\PY{p}{(}\PY{p}{)}

\PY{n}{output\PYZus{}merged\PYZus{}dir} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{results/llama2/final\PYZus{}merged\PYZus{}checkpoint}\PY{l+s+s2}{\PYZdq{}}
\PY{n}{os}\PY{o}{.}\PY{n}{makedirs}\PY{p}{(}\PY{n}{output\PYZus{}merged\PYZus{}dir}\PY{p}{,} \PY{n}{exist\PYZus{}ok}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{model}\PY{o}{.}\PY{n}{save\PYZus{}pretrained}\PY{p}{(}\PY{n}{output\PYZus{}merged\PYZus{}dir}\PY{p}{,} \PY{n}{safe\PYZus{}serialization}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} save tokenizer for easy inference}
\PY{n}{tokenizer} \PY{o}{=} \PY{n}{AutoTokenizer}\PY{o}{.}\PY{n}{from\PYZus{}pretrained}\PY{p}{(}\PY{n}{model\PYZus{}name}\PY{p}{)}
\PY{n}{tokenizer}\PY{o}{.}\PY{n}{save\PYZus{}pretrained}\PY{p}{(}\PY{n}{output\PYZus{}merged\PYZus{}dir}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
Loading checkpoint shards:   0\%|          | 0/2 [00:00<?, ?it/s]
    \end{Verbatim}

    
            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
('results/llama2/final\_merged\_checkpoint/tokenizer\_config.json',
 'results/llama2/final\_merged\_checkpoint/special\_tokens\_map.json',
 'results/llama2/final\_merged\_checkpoint/tokenizer.model',
 'results/llama2/final\_merged\_checkpoint/added\_tokens.json',
 'results/llama2/final\_merged\_checkpoint/tokenizer.json')
\end{Verbatim}
\end{tcolorbox}
        
    完成模型训练后，可以进行推理

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{tokenizer} \PY{o}{=} \PY{n}{AutoTokenizer}\PY{o}{.}\PY{n}{from\PYZus{}pretrained}\PY{p}{(}\PY{n}{model\PYZus{}name}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{tokenizer}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
LlamaTokenizerFast(name\_or\_path='meta-llama/Llama-2-7b-hf', vocab\_size=32000,
model\_max\_length=1000000000000000019884624838656, is\_fast=True,
padding\_side='right', truncation\_side='right', special\_tokens=\{'bos\_token':
AddedToken("<s>", rstrip=False, lstrip=False, single\_word=False,
normalized=False), 'eos\_token': AddedToken("</s>", rstrip=False, lstrip=False,
single\_word=False, normalized=False), 'unk\_token': AddedToken("<unk>",
rstrip=False, lstrip=False, single\_word=False, normalized=False)\},
clean\_up\_tokenization\_spaces=False)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{transformers} \PY{k+kn}{import} \PY{n}{AutoModelForCausalLM}
\PY{n}{output\PYZus{}merged\PYZus{}dir} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{results/llama2/final\PYZus{}merged\PYZus{}checkpoint}\PY{l+s+s2}{\PYZdq{}}

\PY{n}{model} \PY{o}{=} \PY{n}{AutoModelForCausalLM}\PY{o}{.}\PY{n}{from\PYZus{}pretrained}\PY{p}{(}\PY{n}{output\PYZus{}merged\PYZus{}dir}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{model}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
Loading checkpoint shards:   0\%|          | 0/2 [00:00<?, ?it/s]
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
LlamaForCausalLM(
  (model): LlamaModel(
    (embed\_tokens): Embedding(32000, 4096, padding\_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self\_attn): LlamaAttention(
          (q\_proj): Linear(in\_features=4096, out\_features=4096, bias=False)
          (k\_proj): Linear(in\_features=4096, out\_features=4096, bias=False)
          (v\_proj): Linear(in\_features=4096, out\_features=4096, bias=False)
          (o\_proj): Linear(in\_features=4096, out\_features=4096, bias=False)
          (rotary\_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate\_proj): Linear(in\_features=4096, out\_features=11008, bias=False)
          (up\_proj): Linear(in\_features=4096, out\_features=11008, bias=False)
          (down\_proj): Linear(in\_features=11008, out\_features=4096, bias=False)
          (act\_fn): SiLUActivation()
        )
        (input\_layernorm): LlamaRMSNorm()
        (post\_attention\_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm\_head): Linear(in\_features=4096, out\_features=32000, bias=False)
)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model}\PY{p}{,} \PY{n}{tokenizer} \PY{o}{=} \PY{n}{load\PYZus{}model}\PY{p}{(}\PY{n}{output\PYZus{}merged\PYZus{}dir} \PY{p}{,} \PY{n}{bnb\PYZus{}config}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    
    \begin{Verbatim}[commandchars=\\\{\}]
Loading checkpoint shards:   0\%|          | 0/2 [00:00<?, ?it/s]
    \end{Verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
/data/lcl/anaconda3/envs/chinese\_llama2/lib/python3.10/site-
packages/transformers/tokenization\_utils\_base.py:1714: FutureWarning: The
`use\_auth\_token` argument is deprecated and will be removed in v5 of
Transformers.
  warnings.warn(
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{54}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{transformers} \PY{k+kn}{import} \PY{n}{GenerationConfig}

\PY{n}{model}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
\PY{n}{eval\PYZus{}prompt} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+s2}{Below is an instruction that describes a task. Write a response that appropriately completes the request.}

\PY{l+s+s2}{\PYZsh{}\PYZsh{}\PYZsh{} Instruction:}
\PY{l+s+s2}{给我介绍一下北京}
\PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{n}{generation\PYZus{}config} \PY{o}{=} \PY{n}{GenerationConfig}\PY{p}{(}
    \PY{n}{temperature}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
    \PY{n}{top\PYZus{}p}\PY{o}{=}\PY{l+m+mf}{0.75}\PY{p}{,}
    \PY{n}{top\PYZus{}k}\PY{o}{=}\PY{l+m+mi}{40}\PY{p}{,}
    \PY{n}{num\PYZus{}beams}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{,} 
\PY{p}{)}


\PY{c+c1}{\PYZsh{} generating reply}
\PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{inputs} \PY{o}{=} \PY{n}{tokenizer}\PY{p}{(}\PY{n}{eval\PYZus{}prompt}\PY{p}{,} \PY{n}{return\PYZus{}tensors}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{generation\PYZus{}output} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{generate}\PY{p}{(}
        \PY{n}{input\PYZus{}ids}\PY{o}{=}\PY{n}{inputs}\PY{o}{.}\PY{n}{input\PYZus{}ids}\PY{p}{,}
        \PY{n}{generation\PYZus{}config}\PY{o}{=}\PY{n}{generation\PYZus{}config}\PY{p}{,}
        \PY{n}{return\PYZus{}dict\PYZus{}in\PYZus{}generate}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
        \PY{n}{output\PYZus{}scores}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
        \PY{n}{max\PYZus{}new\PYZus{}tokens}\PY{o}{=}\PY{l+m+mi}{4096}\PY{p}{,}
    \PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{回复: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{tokenizer}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{generation\PYZus{}output}\PY{o}{.}\PY{n}{sequences}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]

回复:  <s>
Below is an instruction that describes a task. Write a response that
appropriately completes the request.

\#\#\# Instruction:
给我介绍一下北京

\#\#\# Response:
北京是中国最大的城市，也是中国政治、经济、科技、文化中心。

\#\#\# End:

    \end{Verbatim}


    % Add a bibliography block to the postdoc
    
总的来说，当前高效指令微调优化的技术主要涉及的点有减少参数量、压缩梯度、量化等方式来
来降低计算和减少内存消耗。
这些方法在降低资源占用方面非常有成效，但也存在一定的缺点，
比如精度损失、收敛稳定性等问题。 本文设计的微调技术采用LoRA， 它的原理就是将模型权重分解为低秩分量进行更新,使调优局限在相关任务子空间。
这样做的优势可以减少调优的参数量，降低计算内存。 缺点采用低秩分解操作可能会削弱模型表征能力。
    
\end{document}
